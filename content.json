{"pages":[{"title":"关于我","text":"基本信息 姓名: ? 学历: 在读研究生(国防科技大学计算机专业) 籍贯：重庆 联系我 邮箱: 1034310781@qq.com 博客: https://me.csdn.net/weixin_41134246 知乎: https://www.zhihu.com/people/he-yu-lin-39 github: https://github.com/harrylin-hyl 研究方向 Deep Learning CV Machine Learning 研究项目 人脸识别和检测 医疗病理图像识别 医疗PDL-1细胞检测识别和分割 点监督细胞定位 兴趣方向 AutoML Cloud Point Meta Learning Reinforce Learning 如果有志同道合的朋友可以联系我，共同成长和进步！ 💪💪💪","link":"/about/index.html"}],"posts":[{"title":"Github+Hexo+icarus创建自定义blog","text":"使用Github+Hexo+icarus创建自定义blog 在GitHub上申请个人主页 详情参考：如何在GitHub上面建立自己的个人主页简要介绍Hexo的安装与主题的切换。详细的安装方式与配置请参考官方文档 安装Hexo 需要环境： Node.js &gt;= 8.x Git 了解Hexo： 搭建一个属于自己的个人网站!GitHub+Hexo搭建个人网站 安装Hexo12345npm install hexo-cli -ghexo init blogcd blognpm installhexo server 配置主题cd blog # 使用git将主题下载到themes目录 git clone https://github.com/ppoffice/hexo-theme-icarus.git themes/icarusHexo可以有很多主题，可以在官网中看到很多漂亮的主题，可以根据自己的爱好进行选择，这里我选择了icarus主题，个人感觉简单大方。 FAQ 重新执行hexo serve命令，发现报错如下： ERROR Package cheerio is not installed.ERROR Please install the missing dependencies from the root directory of your Hexo site. 原因缺少cheerio依赖，进入blog-hexo目录，执行npm i cheerio -S命令进行安装即可，-S指安装并将其保存到当前项目的配置中，下次就会统一安装了。 重新执行hexo serve命令，发现如下信息： INFO Checking dependenciesINFO Validating the configuration fileWARN themes/icarus/_config.yml is not found. We are creating one for you…INFO themes/icarus/_config.yml is created. Please restart Hexo to apply changes. 只是提示缺少themes/icarus/_config.yml文件，已经帮助我们生成，再次运行即可正常启动。 个性化设置所谓的个性化设置就是根据个人需要添加不同的插件及功能。基本信息是在主题中的_config.yml进行修改，其他外观布局等分别有各自的文件夹和文件管理。网上有很多的个性设置教程，下面就添加一些优秀的链接： hexo及icarus主题个性定制 Hexo+icarus主题配置","link":"/2020/02/02/github+hexo+icarus%E5%88%9B%E5%BB%BA%E8%87%AA%E5%AE%9A%E4%B9%89blog/"},{"title":"点云规则化处理(一)","text":"本系列是按照bilibili上刘永成博士的分享课的论文笔记，主要针对点云的规则化处理。对于一些有新颖的方法会讲得详细一些。经典的一些针对点云的规则化处理的方法。包括：VoxelNet、SPLATNet和PointCNN 点云是无序且不规则的数据，无法用现有的卷积神经网络进行计算和训练。所以自然就出现将规则化处理和卷积相结合，前面部分用规则化处理，后面部分接上一般的神经网络进行端对端的学习和训练。 VoxelNet2017年苹果公司基于点云的3D物体检测论文“VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection” 具体思想将三维点云划分为一定数量的Voxel，经过点的随机采样（使得较多点的网格采样后点变少，加快运行速度）以及归一化后（减少平移差异），对每一个非空Voxel使用若干个VFE(Voxel Feature Encoding)层进行局部特征提取，得到Voxel-wise Feature，然后经过3D Convolutional Middle Layers进一步抽象特征（增大感受野并学习几何空间表示），最后使用RPN(Region Proposal Network)对物体进行分类检测与位置回归。VoxelNet整个pipeline如下图所示。 核心部分Voxel feature encoding layer对点云划分后基本上是按照pointNet的方式进行特征提取，彩色的表示point feature(也即local feature)，灰色的表示这个划分中的global feature，concatenate之后再进行一个全连接层提取这个划分的总体特征。如图所示： 点云的高效查询此外，由于点云具有高度的稀疏性以及密度不均匀性，作者利用哈希表查询的方式，可以做到快速找到每一个Voxel中的点在三维点云中的具体位置，如图所示： SPLATNetlattice映射到晶格网络+bilateral convolution双边卷积+hash index（SPLATNet, CVPR 2018） SplatNet 网络处理点云的主要motivation： 通过控制点云相应的lattice特征的尺度，利用双边卷积层实现不同分辨率下的点云特征提取，类似经典CNN中的层级特征提。 利用splat 和 slice操作，实现特征（或信号）在2D（图像空间），3D空间（lattice空间）的嵌入转换，从而直观地融合2D图像特征和3D点云特征，进行multi-task 学习。 SplatNet 与 PointNet/PointNet++ 处理点云的不同： Pointnet/Pointnet++，需要设计对称函数（如max-pooling）来处理点云的无序性。SplatNet则不需要，它采用的主要模块BCLBCLBCL中的splat，convolve， slice都直接用到了点云的空间坐标（即相当于保持了一个统一的空间结构关系，具体地，即使采用不同的点云顺序，也会splat到permutohedral lattice空间的相同lattice点点集上）。因此SplatNet可以直接处理无序点云。 博客参考：https://blog.csdn.net/csuwoshikunge/article/details/100829567 PointCNNNIPS 2018 论文《PointCNN: Convolution On x-Transformed Points》 具体思想利用一个KxK的x-Transform矩阵来解决点的无序性，参照卷积的思想对点云进行最远点采样确定中心点，再通过K近邻的方式来近似卷积的特征提取，也可以进行分层次的特征提取，以实现类似卷积的多个size的特征融合。 具体操作分层卷积 如图所示上面部分是正常卷积操作，下面部分是点云卷积操作。进行类比，浅层feature map的宽度大而深度小，深层feature map的宽度小而深度大，拥有更丰富的语义信息。具体确定采样多少个点由最远点采样设置而定，K值也是超参数点需要人为设定，类似于卷积kernel的size。 X-Conv操作 如图所示，K代表Conv的卷积核，$p$代表采样中心点，P是采样中心点的K个最近邻点集，F是输入的点的特征。 第一步：去中心化，移除平移对局部的影响 第二步：提取逐点的特征 第三步：Concatenate操作 第四步：计算KxK的X-transformation矩阵，类似现在用得多的attention矩阵，计算K个点之间的权重。 第五步：相乘 第六步：卷积，可以用可分离卷积、可形变卷积、dialted conv等，提取特征。 PointCNN Architectures 思考 是否考虑在进入x-conv之前采用类似pointNet的T-Net操作消除一部分平移和旋转的影响。 在x-conv的计算过程中多加入类似resnet和densenet的多个size的连接，因为K近邻的方式进行特征提取，势必会有损失，在原始点云中进行多轮不同中心点和K值定义的采样能减少信息损失。 对于点云分布不均匀的方式，KNN的方式不太适用，文中也说这种情况K个点的随机采样要更加优先。","link":"/2020/02/09/%E7%82%B9%E4%BA%91%E8%A7%84%E5%88%99%E5%8C%96%E5%A4%84%E7%90%86(%E4%B8%80)/"},{"title":"PointNet系列方法","text":"PointNet系列处理点云的方法:PointNet、PointNet++、Frustum-PointNet和FlowNet3D 1. PointNet系列 参考：https://zhuanlan.zhihu.com/p/44809266 点云三大性质： 无序性。点云集合顺序不敏感，意味着处理点云数据的模型需要对数据的不同排列保持不变性。目前文献中使用的方法包括将无序的数据重排序、用数据的所有排列进行数据增强然后使用RNN模型、用对称函数来保证排列不变性。由于第三种方式的简洁性且容易在模型中实现，论文作者选择使用第三种方式，既使用maxpooling这个对称函数来提取点云数据的特征。 点与点之间的空间关系。一个物体通常由特定空间内的一定数量的点云构成，也就是说这些点云之间存在空间关系。为了能有效利用这种空间关系，论文作者提出了将局部特征和全局特征进行串联的方式来聚合信息。 不变性。点云数据所代表的目标对某些空间转换应该具有不变性，如旋转和平移。论文作者提出了在进行特征提取之前，先对点云数据进行对齐的方式来保证不变性。对齐操作是通过训练一个小型的网络来得到转换矩阵，并将之和输入点云数据相乘来实现(T-Net)。 2. PointNet PointNet的模型结构如上图所示，其关键流程介绍如下： 输入为一帧的全部点云数据的集合，表示为一个nx3的2d tensor，其中n代表点云数量，3对应xyz坐标。 输入数据先通过和一个T-Net学习到的转换矩阵相乘来对齐，保证了模型的对特定空间转换的不变性。 通过多次mlp对各点云数据进行特征提取后，再用一个T-Net对特征进行对齐。 在特征的各个维度上执行maxpooling操作来得到最终的全局特征。 对分类任务，将全局特征通过mlp来预测最后的分类分数；对分割任务，将全局特征和之前学习到的各点云的局部特征进行串联，再通过mlp得到每个数据点的分类结果。 网络重要组成介绍 T-Net：针对平移或者旋转情况，设计的自适应网络。可学习的一个旋转矩阵，输出$k_1 * k_2$的矩阵，$k_1$是输入层数，$k_2$是输出层数。 MLP：多层感知机。图中有share的地方用的是1x1卷积表示，每层共享权重。没有share部分直接用的全连接层。 分为分类网络和分割网络。分类网络对整个点云分类，分割网络对每一个点分类。顺序和输入点云顺序一致。 3. PointNet++ 网络构成PointNet提取特征的方式是对所有点云数据提取了一个全局的特征，显然，这和目前流行的CNN逐层提取局部特征的方式不一样。受到CNN的启发，作者提出了PointNet++，它能够在不同尺度提取局部特征，通过多层网络结构得到深层特征。PointNet++由以下几个关键部分构成： 采样层（sampling）激光雷达单帧的数据点可以多达100k个，如果对每一个点都提取局部特征，计算量是非常巨大的。因此，作者提出了先对数据点进行采样。作者使用的采样算法是最远点采样（farthest point sampling, FPS），相对于随机采样，这种采样算法能够更好地覆盖整个采样空间。 组合层（grouping）为了提取一个点的局部特征，首先需要定义这个点的“局部”是什么。一个图片像素点的局部是其周围一定曼哈顿距离下的像素点，通常由卷积层的卷积核大小确定。同理，点云数据中的一个点的局部由其周围给定半径划出的球形空间内的其他点构成。组合层的作用就是找出通过采样层后的每一个点的所有构成其局部的点，以方便后续对每个局部提取特征。 特征提取层（feature learning）因为PointNet给出了一个基于点云数据的特征提取网络，因此可以用PointNet对组合层给出的各个局部进行特征提取来得到局部特征。值得注意的是，虽然组合层给出的各个局部可能由不同数量的点构成，但是通过PointNet后都能得到维度一致的特征（由上述K值决定）。 上述各层构成了PointNet++的基础处理模块。如果将多个这样的处理模块级联组合起来，PointNet++就能像CNN一样从浅层特征得到深层语义特征。对于分割任务的网络，还需要将下采样后的特征进行上采样，使得原始点云中的每个点都有对应的特征。这个上采样的过程通过最近的k个临近点进行插值计算得到。 4. Frustum-PointNet 上述的PointNet和PointNet++主要用于点云数据的分类和分割问题，Frustum-PointNet（F-PointNet）将PointNet的应用拓展到了3D目标检测上。目前单纯基于Lidar数据的3D目标检测算法通常对小目标检测效果不佳，为了处理这个问题，F-PointNet提出了结合基于图像的2D检测算法来定位目标，再用其对应的点云数据视锥进行bbox回归的方法来实现3D目标检测。F-PointNet的网络结构如下图所示。可以看到，F-PointNet主要由以下几部分构成： 视锥生成（frustum proposal）：首先通过2D目标检测器来定位图片中的目标以及判断它们的类别。对每一个检测到的目标，通过标定好的传感器的内参和它们之间的转换矩阵得到其对应的点云数据中的各点，即点云视锥。作者使用的2D目标检测模型是基于VGG网络的FPN作为特征提取器，并用Fast R-CNN来预测最终的2D bbox。 3D实例分割（3D instance segmentation）:对每个得到的点云视锥，通过旋转得到以中心视角为坐标轴的点云数据。对转换后的点云数据用PointNet（或PointNet++）进行实例分割。实例分割是一个二分类问题，用于判断每个点属于某个目标或者不属于。 3D边界框回归（3D box estimation）:将上一步实例分割的结果作为mask得到属于某个实例的所有点云，计算其质心作为新的坐标系原点。通过一个T-Net进行回归得到目标质心和当前坐标原点的残差。将点云平移到计算得到的目标质心后，通过PointNet（或PointNet++）对3D bbox的中心、尺寸和朝向进行回归得到最终的输出。此步骤采用的回归方式和Faster R-CNN中类似，不直接回归，而是回归到不同尺寸和朝向的锚点（anchors）。 综上所述，F-PointNet是一个多步骤的3D目标检测算法。如下图所示，为了应对点云数据中各个目标的视角不变性和得到更准确的bbox回归（通过缩小需要回归的值的取值范围），算法需要进行三次坐标转换。模型的loss和2D的目标检测一样是包含分类以及回归的多任务loss。同时，作者提出了一种被称为corner loss的损失函数来对目标的中心、朝向和大小进行联合优化，避免由于某一方面的不准确而主导loss。 详细参考： https://blog.csdn.net/u011507206/article/details/89106892 5. FlowNet3D: Learning Scene Flow in 3D Point Clouds 通过点云预测光流，整个流程如图所示：后融合之后再进行特征聚合输出最后的结果。set_conv用的pointnet++的结构。flow embedding层来进行前后两帧的差异性提取：set_upconv用上采样和前面下采样的特折进行skip操作。","link":"/2020/02/04/PointNet%E7%B3%BB%E5%88%97/"},{"title":"为自定义博客添加评论功能","text":"为自定义博客添加评论功能,介绍基本的方法，主要使用gitalk来实现 介绍一些支持的评论模块具体支持哪些可以看icarus/layout/comment文件夹中的一系列文件。如果要加自己的就要自己添加文件。最开始就想用github来作为用户进行评论，然后看到gitment模块，踩了很多坑，然后强烈推荐gitalk。因为一些原因gitment的服务器不再维护了，所以工作量就变得很大，不建议使用。具体可以看: gitment修复[object ProgressEvent] hexo博客的gitment评论开启一直失败 使用hexo + gitalk参考博客 首先Github上，注册 OAuth Application可以通过这个地址来注册一个应用，按提示填写即可。 成功之后会得到一个client ID 和一个 client secret，在后面的初始化插件时会用到。 修改主题中的_config.yml文件修改coment下面的代码 comment: type: gitalk owner: xxx (harrylin-hyl) # (required) GitHub user name repo: xxx (harrylin-hyl.github.io) # (required) GitHub repository name client_id: xxxxxxx # (required) OAuth application client id 上面生成的 client_secret: xxxxxx # (required) OAuth application client secret 上面生成的 admin: xxx (harrylin-hyl) #此账户一般为用户名 GitHub user name 文章中能创建issue需要此用户登录才可以，点了创建issue后刷新一遍才能看到！！！！ create_issue_manually: true distraction_free_mode: true 重新刷新一下，重启一下hexo就可以了，以下是效果图：","link":"/2020/02/02/%E4%B8%BA%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0%E8%AF%84%E8%AE%BA%E5%8A%9F%E8%83%BD/"},{"title":"点云规则化处理(二)","text":"本系列是按照bilibili上刘永成博士的分享课的论文笔记，主要针对点云的规则化处理。对于一些有新颖的方法会讲得详细一些。继续讲一些经典的点云的规则化处理的方法。包括：PointSift、SO-Net、Pointwise CNN、Kd-Net和FCPN PointShiftPointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic Segmentation 具体思想将sift算子的思想用在点云中。主要是两个方面：1.方向编码。8个重要方向的信息通过一个方向编码单元（orientation-encoding unit）来提取特征。2.通过堆叠多个尺度下的方向编码单元（orientation-encoding unit），以获得尺度不变性。 核心部分方向编码卷积（orientation-encoding convolution） 如图所示，包含以下几个部分： 给定点$p_0$，其对应的特征为$f_0$。以$p_0$为中心点的3D空间中，可以根据8个方向分为8个子空间（octant）。从其中分别寻找最近邻点，如果在某个子空间（octant）内，搜索半径$r$内没有找到点，就将这个子空间的特征认为等于$f_0$。采样之后原本一个中心点d维度的特征变为了2x2x2xd的特征，表示八个象限对应的特征向量。 为了使卷积操作能感知方向上的信息，分别在X、Y、Z轴上进行三阶段的卷积。使用的卷积核为[1,2]，stride为[1,2]。 在三次卷积后，每个点都被转换为了一个d维的向量。 尺度感知结构（scale-aware architecture） 经过多个OE unit单位提取特征之后用shortcut的方式，对多个尺度的特征进行concatenate，保证尺度不变性。 网络整体结构 PointSift模块表示前文的尺度感知结构，SA模块用的pointNet++的下采样模块，FP模块用的pointNet++的上采样模块。 SO-NetSO-Net: Self-Organizing Network for Point Cloud Analysis( SO-Net, CVPR 2018) 具体思想将SOM(自组织映射)的想法应用到点云规则化处理上。基本上是应用SOM替换了点云采样这一步，使用自组织映射来模拟点云分布，达到降维和确定关键点的目的。 核心部分主要是讲SOM部分，后面部分基本上跟常用方法差不多。 SOM(自组织映射) 如图所示尽可能找到关键点拟合原始数据，用SOM的方式替换点云采样的一系列方法(最远点采样等)。 SOM初始化 算法的过程如第一幅图所示，效果为第二幅图所示。关键想法是在任何一对节点之间施加排斥力，并且将节点逼近原始数据点。 Batch Update Training 如图所示，w是服从高斯分布的，经过迭代SOM中的点会拟合原始数据，并且具有表示原始数据的能力(降维)。 整体框架 后续部分和其他方法类似。定义knn的k值，进行平均化。这个地方的每个代表点都有k个点的特征，一共就有kN个点的特征都放入共享参数的全连接层中进行特征提取。后面将特征进行max-pooling又可以筛出掉冗余特征留下代表性点的特征，即又一个SOM层。 Pointwise CNN(CVPR 2018)简单暴力，直接用3x3的网格，求和+归一化，当作一个特征去卷积。 Kd-Net(ICCV 2017)参考博客：https://blog.csdn.net/hongbin_xu/article/details/93915673 具体思想用Kd-Tree对原始点云做划分，通过变换的学习到达根节点。将数据结构用到点云分割里。 图1是KD-Netwoks的示意图。图中只考虑二维的情况。 在一个有8个点的点云上建立KD树，在每一层都从X/Y轴选择一个维度进行划分。 从根节点到叶节点对KD树的每个节点进行编号，根节点编号为0，往后递增。 KD数最左边的8到15号节点实际就是原始点云，右边的0号节点可以看做输出的类别预测 从左到右，可以看做这个KD-Network的特征前向传播的方向。 图中的圆圈表示的是特征传播并聚合的过程，对应卷积核，其参数是可学习的。相同的颜色表示共享参数，即如果是在相同维度对点集进行划分，认为其卷积参数也应该共享。 现实的点云数据是三维的，所以有三个方向的划分。而划分面可以看作是决策面，区分两个部分点云，非叶子节点的参数相当于也是决策面的参数。对于每一个点云样本都需要提前建立Kd-tree。这种方法也可以用自编码的方式来实现点云分割，如图所示： FCPN(ECCV 2018)Fully-Convolutional Point Networks for Large-Scale Point Clouds 提出了一个能有效处理大规模3D数据的全卷积网络——FCPN，该网络将无规则的3D输入数据比如点云，转换为内部有序的数据结构，然后再使用3D卷积进行处理。与输入输出结构一致的传统方法相比，FCPN具有在高效存储输入数据中操作的优点，同时利用自然结构的卷积运算以避免对冗余空间信息的计算与存取。该网络消除了对原始数据预处理和后处理的需求，加之全卷积结构的特点，使其能够端到端处理大规模空间的点云数据。FCPN另一个优点是能够直接从输入点云生成有序的输出或者预测图，因此使其能够作为很多3D任务的通用点云描述器。 具体思想 3D grid + PointNet + 3D CNN 模仿2D空间的FCN，用3D grid, 用pointnet学习每个grid的low-level特征，再用3D CNN做分割 可以一次性处理200k个点","link":"/2020/02/13/%E7%82%B9%E4%BA%91%E8%A7%84%E5%88%99%E5%8C%96%E5%A4%84%E7%90%86(%E4%BA%8C)%20/"},{"title":"点云采样","text":"有关点云采样的方法： 格点采样、均匀采样、几何采样 格点采样格点采样，就是把三维空间用格点离散化，然后在每个格点里采样一个点。具体方法如下： 创建格点：如中间图所示，计算点云的包围盒，然后把包围盒离散成小格子。格子的长宽高可以用户设定，也可以通过设定包围盒三个方向的格点数来求得。 每个小格子包含了若干个点，取离格子中心点最近的点为采样点，如右图所示。 格点采样的特点： 效率非常高 采样点分布比较均匀，但是均匀性没有均价采样高 可以通过格点的尺寸控制点间距 不能精确控制采样点个数 均匀采样均匀采样的方法有很多，并且有一定的方法来评估采样的均匀性。这里介绍一种简单的均匀采样方法，最远点采样。具体方法如下： 输入点云记为C，采样点集记为S，S初始化为空集。 随机采样一个种子点Seed，放入S。如图1所示。 每次采样一个点，放入S。采样的方法是，在集合C-S里，找一点距离集合S距离最远的点。其中点到集合的距离为，这点到集合里所有点距最小的距离。如图2-6所示，采样点S的数量分别为2，4，10，20，100. 最远点采样的特点： 采样点分布均匀 算法时间复杂度有些高，因为每次采样一个点，都要计算集合到集合之间的距离。可以采用分治的方法来提高效率。 采样点一般先分布在边界附近，这个性质在有些地方是有用的，比如图元检测里面的点采样。 几何采样几何采样，在点云曲率越大的地方，采样点个数越多。下面介绍一种简单的几何采样方法，具体方法如下： 输入是一个点云，目标采样数S，采样均匀性U 点云曲率计算比较耗时，这里我们采用了一个简单方法，来近似达到曲率的效果：给每个点计算K邻域，然后计算点到邻域点的法线夹角值。曲率越大的地方，这个夹角值就越大。 设置一个角度阈值，比如5度。点的邻域夹角值大于这个阈值的点，被放入几何特征区域G。这样点云就分成了两部分，几何特征区域G和其它区域。 均匀采样几何特征区域G和其它区域，采样数分别为S * (1 - U)，S * U。 这个采样方法的特点： 几何特征越明显的区域，采样点个数分布越多 计算效率高 采样点局部分布是均匀的 稳定性高：通过几何特征区域的划分，使得采样结果抗噪性更强","link":"/2020/02/01/%E7%82%B9%E4%BA%91%E9%87%87%E6%A0%B7/"},{"title":"语义分割中的损失","text":"有关图像语义分割的一系列改进loss 常用loss: CE BCE WCE Focal loss Dice loss IOU loss Tversky loss 敏感性–特异性 loss Generalized Dice loss BCE + Dice loss Dice + Focal loss Exponential Logarithmic loss以上部分有一篇博客讲得比较清楚：原文博客链接: (https://blog.csdn.net/m0_37477175/article/details/83004746) Lov´asz-Softmaxloss请看我的另一篇博客：（https://blog.csdn.net/weixin_41134246/article/details/103280203）对采样进行改进的loss Boundary loss 提出generalized Dice loss，对Dice loss在计算中加上权重，主要是为了解决样本不平衡问题。 根据这一个权重函数可以看出来，$r_{ln}$代表这一个类的label map之和，当这一个类别像素值少的时候权重会变大，像素值大的时候权重会变小。 主要还是应用权重来解决不平衡的问题。 还有一个部分是边界损失 想法比较简单，用groundtruth计算出边界部分，然后在预测值对这一部分进行损失计算，对各像素损失求和。 代码链接：（https://github.com/LIVIAETS/surface-loss） ConservativeLoss 核心思想是为了在不同领域的适应能力中，有较强的泛化性。对表现极度好的结果进行惩罚，从而鼓励中等表现得结果。如图所示，根据置信度来计算CL loss，相应得进行惩罚。$\\lambda$为5， $a$为$e$。 combo loss代码地址：https://github.com/asgsaeid/ComboLoss/blob/master/combo_loss.pydice loss + weighted ce$\\beta$控制 假阳性和假阴性的权重，大于0.5惩罚假阳性，小于0.5惩罚假阴性。$a$控制dice loss 和weighted ce loss的权重。 边界部分有权重的loss Active Contour Loss代码地址：（https://github.com/xuuuuuuchen/Active-Contour-Loss/blob/master/Active-Contour-Loss.py） - 最终的loss是Length 和Region 的和，代码中$\\lambda$设为的1，这是一个超参数。 - 首先看一下整个损失，它的计算方式都是用的差的平方的形式。 - 然后Length loss部分，只用了pred的信息，最小化梯度的大小，在这里我的理解是，它想达到的目的是让物体的内部尽量平滑，只有边界部分有这个梯度的变化。 - 接着是Region部分，使用label的监督信息对每一个点进行监督训练。 - 总结一下整个地方，感觉在边界部分，这两个Loss有冲突的地方，Length损失在边界部分会变大，但是它因为没有监督信息，它趋向于对所有位置都平滑处理，但是Region部分有监督信息，为了让边界部分损失变小会将物体和边界分开。也因为是用的差的平方的形式，边界部分的预测值概率应该是平滑下降的，骤降的loss太大，也就是惩罚太大。感觉是对整个预测结果作了一个平滑处理。 Distance Map Loss Penalty Term for Semantic Segmentation$\\phi$代表 边界的距离图，如图所示：结合上面的公式，就代表着边界部分的权重会高于其他地方，其他的没有改动。 NonAdjacencyLoss代码地址：https://github.com/trypag/NonAdjLoss创新点是加入了各个类之间的邻接矩阵，用一个子网络去预测这个矩阵。即可以监督学习也可以半监督的学习。前面一部分是一般的损失，如dice或交叉熵等，后面是用邻接矩阵加的限定损失。 求$a_{ij}\\left( \\phi \\right)$这个过程是离线的过程，根据总的有标签的数据求一个最后求和的邻接矩阵“模板”。形成一个先验的知识，对于半监督无标签时，这个G(w)这一项的损失也可以根据这个“模板”进行计算。对于医疗图像来说，这样做也是有一定道理的。但是我自己认为这样做会无法考虑到空间中的信息，只是考虑了类与类之间的信息。","link":"/2020/02/04/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E4%B8%AD%E7%9A%84loss%E6%80%BB%E7%BB%93/"}],"tags":[{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"github","slug":"github","link":"/tags/github/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"点云","slug":"点云","link":"/tags/%E7%82%B9%E4%BA%91/"},{"name":"图像语义分割","slug":"图像语义分割","link":"/tags/%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"}],"categories":[{"name":"自定义博客","slug":"自定义博客","link":"/categories/%E8%87%AA%E5%AE%9A%E4%B9%89%E5%8D%9A%E5%AE%A2/"},{"name":"点云","slug":"点云","link":"/categories/%E7%82%B9%E4%BA%91/"},{"name":"图像语义分割","slug":"图像语义分割","link":"/categories/%E5%9B%BE%E5%83%8F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"}]}